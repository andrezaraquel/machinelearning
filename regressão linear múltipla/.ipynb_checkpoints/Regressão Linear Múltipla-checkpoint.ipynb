{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Múltipla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regressão Múltipla com sklearn\n",
    "\n",
    "Neste documento, a ideia é criar um modelo de regressão linear múltipla. Esse modelo será ajustado aos dados de notas de alunos de computação da UFCG em algumas disciplinas do primeiro período. A última coluna é a variável alvo representando o CRA final depois de concluir o curso. As outras colunas são algumas disciplinas do primeiro período. O pressuposto aqui é que as notas em disciplinas no primeiro período ajudam a explicar o CRA final dos alunos de computação.\n",
    "\n",
    "Primeiramente, vou usar a biblioteca *sklearn* para criar o modelo. Depois de criado o modelo linear, a função *fit()* o ajusta aos dados (**X** contém todas as colunas de notas dos alunos e **Y** é o vetor com a coluna do cra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1570.723295211792 ms intercept = [1.73771151], linear_coefficients = [[0.10304143 0.0464367  0.16409834 0.38117843 0.02027816]] error (R²) = 0.423803240951211\n"
     ]
    }
   ],
   "source": [
    "grades = np.genfromtxt(\"sample_students_grades.csv\", delimiter=\",\", skip_header=1)\n",
    "\n",
    "X = grades[:,[0,1,2,3,4]]\n",
    "Y = grades[:,5][:,np.newaxis]\n",
    "\n",
    "startTime = time.time()\n",
    "linear_reg = linear_model.LinearRegression() # this creates a linear regression object\n",
    "linear_reg.fit(X,Y) #this function fits a linear model\n",
    "\n",
    "endTime = time.time()\n",
    "#print(linear_reg.score(X,Y)) #R² error\n",
    "#print(linear_reg.coef_) # w1, w2, w3, ...\n",
    "#print(linear_reg.intercept_) # w0\n",
    "print(\"After {0} ms intercept = {1}, linear_coefficients = {2} error (R²) = {3}\".format(str(1000*(endTime-startTime)), linear_reg.intercept_, linear_reg.coef_, linear_reg.score(X,Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regressão Múltipla \"do zero\"\n",
    "\n",
    "Agora, farei um modelo e o ajustarei aos dados sem utilizar a biblioteca. Ao final, compararei os coeficientes do novo modelo com os coeficientes do modelo criado anteriormente. Isso será o teste que indicará se o novo algoritmo está funcionando corretamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mean Squared Error\n",
    "\n",
    "$MSE(\\hat{w})=\\frac{1}{N}(y-\\hat{\\mathbf{w}}^T\\mathbf{x})^T(y-\\hat{\\mathbf{w}}^T\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Mean Squared Error\n",
    "def compute_mse_vectorized(w,X,Y):\n",
    "    res = Y - np.dot(X, w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return totalError / float(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def step_gradient_vectorized(w_current,X,Y,learningRate):\n",
    "    \n",
    "    res = Y - np.dot(X, w_current) # (y−Hw)\n",
    "   \n",
    "    gradients = np.multiply(-2, np.dot(X.T, res)) # -2H.T (y−Hw)\n",
    "    \n",
    "    rate = np.multiply(learningRate, gradients)  # α * -2H.T (y−Hw)  \n",
    "    \n",
    "    new_w = w_current - rate # w(t) − α * -2H.T (y−Hw)\n",
    "    \n",
    "    return [new_w,gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    gradients = np.array([np.inf])\n",
    "    i = 0\n",
    "    while (np.linalg.norm(gradients)>=epsilon):\n",
    "        w,gradients = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        \n",
    "        #print(np.linalg.norm(gradients))\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"MSE in the iteration {0} is equals to {1}\".format(i,compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    print(\"It converged with {0} iterations.\".format(i))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main\n",
    "\n",
    "Como pode-se observar pela saída, o código converge com 212513 iterações quando os parâmetros são: **learning_rate = 0.0005** e **epsilon = 0.5**. Como pode ser visto no código acima (função *step_gradient_vectorized*), a taxa de aprendizado é dividida pelo tamanho de X. Não entendi muito bem o motivo, mas só convergiu dessa forma.\n",
    "\n",
    "Depois de 2047,2838878631592 ms, w0 = 0,92369587, w1 = 0,111762742, w2 = 0,08368884, w3 = 0,16224365, w4 = 0,42251334  e w5 = 0,03029537, com erro de 0,41597456. \n",
    "\n",
    "Como é possível observar, esses valores se aproximam dos valores retornados pelo modelo criado com a biblioteca sklearn que foram: w0 =1.73771151, w1 = 0.10304143, w2 = 0.0464367, w3 = 0.16409834 w4 = 0.38117843 e w5 = 0.02027816, com erro de 0.423803240951211;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at w = [[0. 0. 0. 0. 0. 0.]], error = [[54.47995386]]\n",
      "Running...\n",
      "MSE in the iteration 0 is equals to [[15.39415211]]\n",
      "MSE in the iteration 1000 is equals to [[0.43036269]]\n",
      "MSE in the iteration 2000 is equals to [[0.42891282]]\n",
      "MSE in the iteration 3000 is equals to [[0.42766679]]\n",
      "MSE in the iteration 4000 is equals to [[0.42650933]]\n",
      "MSE in the iteration 5000 is equals to [[0.42543391]]\n",
      "MSE in the iteration 6000 is equals to [[0.42443472]]\n",
      "MSE in the iteration 7000 is equals to [[0.42350635]]\n",
      "MSE in the iteration 8000 is equals to [[0.42264379]]\n",
      "MSE in the iteration 9000 is equals to [[0.42184238]]\n",
      "MSE in the iteration 10000 is equals to [[0.42109776]]\n",
      "MSE in the iteration 11000 is equals to [[0.42040593]]\n",
      "MSE in the iteration 12000 is equals to [[0.41976314]]\n",
      "MSE in the iteration 13000 is equals to [[0.41916591]]\n",
      "MSE in the iteration 14000 is equals to [[0.41861102]]\n",
      "MSE in the iteration 15000 is equals to [[0.41809545]]\n",
      "MSE in the iteration 16000 is equals to [[0.41761644]]\n",
      "MSE in the iteration 17000 is equals to [[0.41717137]]\n",
      "MSE in the iteration 18000 is equals to [[0.41675786]]\n",
      "MSE in the iteration 19000 is equals to [[0.41637365]]\n",
      "MSE in the iteration 20000 is equals to [[0.41601668]]\n",
      "MSE in the iteration 21000 is equals to [[0.41568501]]\n",
      "MSE in the iteration 22000 is equals to [[0.41537685]]\n",
      "It converged with 22992 iterations.\n",
      "After 252.60448455810547 ms w = [[1.00514726 0.11616793 0.07996134 0.16242923 0.41837732 0.02929304]], error = [[0.41509302]]\n"
     ]
    }
   ],
   "source": [
    "grades_incremented = np.c_[np.ones(len(grades)),grades]\n",
    "\n",
    "X = grades_incremented[:,[0,1,2,3,4,5]] # grades without cra\n",
    "Y = grades_incremented[:,6][:,np.newaxis] # cra\n",
    "init_w = np.zeros((6,1)) # vector of coefficients\n",
    "\n",
    "learning_rate = 0.00003\n",
    "epsilon = 0.9\n",
    "\n",
    "print(\"Starting gradient descent at w = {0}, error = {1}\".format(init_w.T, compute_mse_vectorized(init_w, X,Y)))\n",
    "print(\"Running...\")\n",
    "startTime = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X,Y, learning_rate, epsilon)\n",
    "endTime = time.time()\n",
    "print(\"After {0} ms w = {1}, error = {2}\".format(str(1000*(endTime-startTime)), w.T, compute_mse_vectorized(w,X,Y)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
