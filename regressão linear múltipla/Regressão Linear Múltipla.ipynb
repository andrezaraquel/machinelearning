{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Múltipla\n",
    "\n",
    "Nessa tarefa você vai estender sua implementação da tarefa passada para considerar múltiplas variáveis. Você pode estender a versão vetorizada implementada neste notebook para regressão simples. \n",
    "\n",
    "- Rode o algoritmo com \"sample_students_grades.csv\", onde as linhas representam as notas de alunos de computação de alunos da UFCG em algumas disciplinas do primeiro período. A última coluna é a variável alvo representando o CRA final depois de concluir o curso. As outras colunas são algumas disciplinas do primeiro período. O pressuposto aqui é que as notas em disciplinas no primeiro período ajudam a explicar o CRA final dos alunos de computação.\n",
    "\n",
    "- Compare o valor dos coeficientes estimados pelo seu algoritmo com o valor dos coeficientes da regressão linear do scikit learn para testar se o seu algoritmo está funcionando corretamente.\n",
    "\n",
    "A entrega deve ser o link no seu github para o notebook Jupyter com código python e texto explicativo quando necessário. De preferência, crie um repositório na sua conta do github e envie o link do html do notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_vectorized(w,X,Y):\n",
    "    res = Y - np.dot(X,w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return totalError / float(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current,X,Y,learningRate):\n",
    "    res = Y - np.dot(X,w_current)\n",
    "    b_gradient = np.sum(res)\n",
    "    X = X[:,1][:,np.newaxis]\n",
    "    m_gradient = np.sum(np.multiply(res,X))\n",
    "    new_w = np.array([(w_current[0] + (2 * learningRate * b_gradient)),\n",
    "             (w_current[1] + (2 * learningRate * m_gradient))])\n",
    "    return [new_w,b_gradient,m_gradient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    grad = np.array([np.inf,np.inf])\n",
    "    i = 0\n",
    "    while (np.linalg.norm(grad)>=epsilon):\n",
    "        w,b_gradient,m_gradient = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        grad = np.array([b_gradient,m_gradient])\n",
    "        #print(np.linalg.norm(grad))\n",
    "        if i % 1000 == 0:\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i,compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          8.7        10.          9.          9.1         8.4\n",
      "   8.47764706]\n",
      " [ 1.          7.          7.          7.7         7.          6.2\n",
      "   6.85172414]\n",
      " [ 1.          8.6         9.8         7.9         9.6         8.7\n",
      "   9.09058824]\n",
      " [ 1.          7.8         8.3         6.8         8.2         8.\n",
      "   7.28351648]\n",
      " [ 1.          5.2         9.3         5.          8.5         5.\n",
      "   7.20574713]\n",
      " [ 1.          6.1         9.2         9.1         9.3         5.6\n",
      "   7.80823529]\n",
      " [ 1.          6.3         9.6         8.3         8.6         6.7\n",
      "   8.85882353]\n",
      " [ 1.          7.5         8.9         7.5         7.9         5.\n",
      "   6.15882353]\n",
      " [ 1.          6.6         9.          5.          7.9         5.\n",
      "   6.73058824]\n",
      " [ 1.          7.5         6.5         7.3         9.2         8.2\n",
      "   7.07931034]\n",
      " [ 1.          6.6         7.9         8.4         7.5         8.3\n",
      "   6.38737864]\n",
      " [ 1.          8.4        10.          9.3         9.6         7.\n",
      "   6.13451327]\n",
      " [ 1.          5.6         7.3         6.7         8.1         5.\n",
      "   6.83406593]\n",
      " [ 1.          9.7         9.1         7.2         8.3         5.\n",
      "   7.24719101]\n",
      " [ 1.          7.4         7.5         6.5         7.3         5.4\n",
      "   6.11627907]\n",
      " [ 1.          7.2         9.5         8.9         8.1         5.8\n",
      "   8.52235294]\n",
      " [ 1.          8.8         8.4         9.8         9.3         7.\n",
      "   8.77176471]\n",
      " [ 1.          7.          9.6         8.6         9.3         7.5\n",
      "   8.66117647]\n",
      " [ 1.          5.8        10.          8.2         7.4         7.2\n",
      "   7.01058824]\n",
      " [ 1.          7.          7.2         5.3         7.6         5.\n",
      "   7.1247191 ]\n",
      " [ 1.          7.          7.5         5.6         8.          5.6\n",
      "   7.05862069]\n",
      " [ 1.          9.2         8.3         8.6         8.4         7.6\n",
      "   8.77294118]\n",
      " [ 1.          7.2         8.5         7.7         8.4         5.\n",
      "   7.72588235]\n",
      " [ 1.          5.6         8.          5.          8.5         8.2\n",
      "   6.46703297]\n",
      " [ 1.          7.3         8.3         7.          7.7         7.2\n",
      "   7.08315789]\n",
      " [ 1.          7.          8.7         6.5         8.1         5.\n",
      "   6.60481928]\n",
      " [ 1.          8.8         7.3         6.2         8.3         5.\n",
      "   7.21241379]\n",
      " [ 1.          8.3         9.6         7.          9.1         5.1\n",
      "   7.99411765]\n",
      " [ 1.          7.          9.6         8.4         8.2         9.\n",
      "   7.96321839]\n",
      " [ 1.          6.1         9.8         8.          8.8         5.\n",
      "   8.03218391]\n",
      " [ 1.          7.2         8.8         8.4         7.8         5.4\n",
      "   6.51075269]\n",
      " [ 1.          8.1         9.          7.8         7.9         5.\n",
      "   8.2137931 ]\n",
      " [ 1.          7.3         9.          9.2         7.4         7.\n",
      "   7.34      ]\n",
      " [ 1.          6.3         7.8         7.1         6.3         7.7\n",
      "   7.14470588]\n",
      " [ 1.          5.          7.3         5.6         5.9         5.\n",
      "   5.95940594]\n",
      " [ 1.          8.3         8.6         5.5         7.1         5.1\n",
      "   5.63539823]\n",
      " [ 1.          8.3         9.4         9.6         7.4         7.9\n",
      "   7.34712644]\n",
      " [ 1.          7.5         7.          8.          8.          5.2\n",
      "   6.57857143]\n",
      " [ 1.          7.          9.          5.9         6.          7.\n",
      "   6.33647059]\n",
      " [ 1.          7.5         9.4         8.1         9.2         6.5\n",
      "   8.53058824]\n",
      " [ 1.          5.3         9.5         9.1         8.          5.\n",
      "   7.43483146]\n",
      " [ 1.          9.5         8.5         5.          7.5         5.4\n",
      "   7.05955056]\n",
      " [ 1.          7.2         7.75        5.7         7.          5.2\n",
      "   6.81034483]\n",
      " [ 1.          8.3         9.3         5.          7.2         5.\n",
      "   7.58235294]\n",
      " [ 1.          7.3         9.6         7.7         9.          5.7\n",
      "   7.59294118]\n",
      " [ 1.          5.1         9.6         7.8         8.4         5.6\n",
      "   6.84395604]\n",
      " [ 1.          7.          9.1         8.5         8.8         5.\n",
      "   7.92352941]\n",
      " [ 1.          5.4         9.          8.5         9.3         5.9\n",
      "   7.44022989]\n",
      " [ 1.          7.5         9.5         8.6         8.5         8.\n",
      "   8.27058824]\n",
      " [ 1.          5.9         8.          6.2         8.4         5.7\n",
      "   7.79647059]\n",
      " [ 1.          8.8         8.3         6.6         7.          5.1\n",
      "   4.87446809]\n",
      " [ 1.          7.2         7.6         7.9         7.7         6.1\n",
      "   7.4954023 ]\n",
      " [ 1.          7.          8.5         8.1         7.3         5.\n",
      "   6.78314607]\n",
      " [ 1.          7.2         9.1         8.2         8.9         5.2\n",
      "   7.22580645]\n",
      " [ 1.          5.          7.          8.          7.2         5.\n",
      "   7.93373494]\n",
      " [ 1.          7.8         9.2         7.6         9.8         6.3\n",
      "   8.30941176]\n",
      " [ 1.          8.6         9.6         8.6         9.5         7.5\n",
      "   8.43975904]\n",
      " [ 1.          8.2         9.6         8.8         8.4         7.\n",
      "   8.24705882]\n",
      " [ 1.          8.7         8.          8.3         8.2         7.5\n",
      "   7.36853933]\n",
      " [ 1.          5.9         7.          6.1         7.8         5.1\n",
      "   6.63493976]\n",
      " [ 1.          7.2         7.7         8.3         7.9         5.\n",
      "   8.02235294]\n",
      " [ 1.          8.7         8.7         8.1         8.9         5.2\n",
      "   7.44252874]\n",
      " [ 1.          7.2         9.2         7.          8.8         5.7\n",
      "   7.50117647]\n",
      " [ 1.          6.8         8.2         7.1         8.9         5.3\n",
      "   6.00093458]\n",
      " [ 1.          8.1         9.3         8.          7.5         5.5\n",
      "   7.76117647]\n",
      " [ 1.          7.5         8.5         7.3         8.6         7.\n",
      "   7.68470588]\n",
      " [ 1.          7.1         9.1         9.5         7.9         7.\n",
      "   6.76363636]\n",
      " [ 1.          7.3         7.          8.3         7.          7.\n",
      "   6.92916667]\n",
      " [ 1.          7.          8.          7.          8.6         5.3\n",
      "   7.86987952]\n",
      " [ 1.          5.8         9.3         7.8         8.2         6.4\n",
      "   7.72068966]\n",
      " [ 1.          5.1         6.2         5.          8.          6.\n",
      "   7.26746988]\n",
      " [ 1.          8.2         6.2         5.4         6.7         5.1\n",
      "   7.25057471]\n",
      " [ 1.          9.8         7.6        10.         10.          8.5\n",
      "   8.85058824]\n",
      " [ 1.          6.2         7.5         8.1         7.2         8.6\n",
      "   6.41584158]\n",
      " [ 1.          8.7         8.2         8.1         9.9         8.3\n",
      "   8.80235294]\n",
      " [ 1.          6.2        10.          5.          7.8         8.\n",
      "   5.7588785 ]\n",
      " [ 1.          7.8         7.3         9.1         9.8         8.\n",
      "   7.66923077]\n",
      " [ 1.          5.2         7.7         5.4         7.5         7.5\n",
      "   5.58165138]\n",
      " [ 1.          6.7         9.3         7.4         8.3         5.1\n",
      "   7.20786517]\n",
      " [ 1.          5.3         7.6         7.4         9.          5.8\n",
      "   6.90526316]\n",
      " [ 1.          7.2         8.3         7.8         8.1         9.3\n",
      "   6.93195876]\n",
      " [ 1.          9.5         8.          6.          7.1         7.\n",
      "   7.40117647]\n",
      " [ 1.          5.6         7.          6.6         8.6         5.2\n",
      "   6.99550562]\n",
      " [ 1.          7.1        10.          5.1         7.          7.3\n",
      "   6.9862069 ]\n",
      " [ 1.          8.          7.8         5.          8.2         8.2\n",
      "   7.10224719]\n",
      " [ 1.          5.          7.2         7.8         7.4         5.3\n",
      "   7.28202247]\n",
      " [ 1.          9.5         8.          8.9         9.8         8.1\n",
      "   8.92470588]\n",
      " [ 1.          6.1         8.6         7.4         8.3         5.\n",
      "   7.3       ]]\n",
      "Starting gradient descent at w0 = [0.], w1 = [0.], error = [[72.87059659]]\n",
      "Running...\n",
      "MSE na iteração 0 é de [[2.68995652]]\n",
      "MSE na iteração 1000 é de [[1.55440894]]\n",
      "MSE na iteração 2000 é de [[1.17511577]]\n",
      "MSE na iteração 3000 é de [[1.03095375]]\n",
      "MSE na iteração 4000 é de [[0.97616055]]\n",
      "MSE na iteração 5000 é de [[0.95533471]]\n",
      "MSE na iteração 6000 é de [[0.94741922]]\n",
      "MSE na iteração 7000 é de [[0.94441069]]\n",
      "After 251.85656547546387 ms w0 = [7.53671199], w1 = [0.13024664], error = [[0.94373906]]\n"
     ]
    }
   ],
   "source": [
    "grades = np.genfromtxt(\"sample_students_grades.csv\", delimiter=\",\", skip_header=1)\n",
    "grades = np.c_[np.ones(len(grades)),grades]\n",
    "X = grades[:,[0,1]]\n",
    "print(grades)\n",
    "Y = grades[:,2][:,np.newaxis]\n",
    "init_w = np.zeros((2,1))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "#num_iterations = 10000\n",
    "epsilon = 0.5\n",
    "print(\"Starting gradient descent at w0 = {0}, w1 = {1}, error = {2}\".format(init_w[0], init_w[1], compute_mse_vectorized(init_w, X,Y)))\n",
    "print(\"Running...\")\n",
    "startTime = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X,Y, learning_rate, epsilon)\n",
    "endTime = time.time()\n",
    "print(\"After {0} ms w0 = {1}, w1 = {2}, error = {3}\".format(str(1000*(endTime-startTime)), w[0], w[1], compute_mse_vectorized(w,X,Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
