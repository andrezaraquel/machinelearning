{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Múltipla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regressão Múltipla com sklearn\n",
    "\n",
    "Neste documento, a ideia é criar um modelo de regressão linear múltipla. Esse modelo será ajustado aos dados de notas de alunos de computação da UFCG em algumas disciplinas do primeiro período. A última coluna é a variável alvo representando o CRA final depois de concluir o curso. As outras colunas são algumas disciplinas do primeiro período. O pressuposto aqui é que as notas em disciplinas no primeiro período ajudam a explicar o CRA final dos alunos de computação.\n",
    "\n",
    "Primeiramente, vou usar a biblioteca *sklearn* para criar o modelo. Depois de criado o modelo linear, a função *fit()* o ajusta aos dados (**X** contém todas as colunas de notas dos alunos e **Y** é o vetor com a coluna do cra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1.0075569152832031 ms intercept = [1.73771151], linear_coefficients = [[0.10304143 0.0464367  0.16409834 0.38117843 0.02027816]] error (R²) = 0.423803240951211\n"
     ]
    }
   ],
   "source": [
    "grades = np.genfromtxt(\"sample_students_grades.csv\", delimiter=\",\", skip_header=1)\n",
    "\n",
    "X = grades[:,[0,1,2,3,4]]\n",
    "Y = grades[:,5][:,np.newaxis]\n",
    "\n",
    "startTime = time.time()\n",
    "linear_reg = linear_model.LinearRegression() # this creates a linear regression object\n",
    "linear_reg.fit(X,Y) #this function fits a linear model\n",
    "\n",
    "endTime = time.time()\n",
    "#print(linear_reg.score(X,Y)) #R² error\n",
    "#print(linear_reg.coef_) # w1, w2, w3, ...\n",
    "#print(linear_reg.intercept_) # w0\n",
    "print(\"After {0} ms intercept = {1}, linear_coefficients = {2} error (R²) = {3}\".format(str(1000*(endTime-startTime)), linear_reg.intercept_, linear_reg.coef_, linear_reg.score(X,Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regressão Múltipla \"do zero\"\n",
    "\n",
    "Agora, farei um modelo e o ajustarei aos dados sem utilizar a biblioteca. Ao final, compararei os coeficientes do novo modelo com os coeficientes do modelo criado anteriormente. Isso será o teste que indicará se o novo algoritmo está funcionando corretamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mean Squared Error\n",
    "\n",
    "$MSE(\\hat{w})=\\frac{1}{N}(y-\\hat{\\mathbf{w}}^T\\mathbf{x})^T(y-\\hat{\\mathbf{w}}^T\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Mean Squared Error\n",
    "def compute_mse_vectorized(w,X,Y):\n",
    "    res = Y - np.dot(X, w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return totalError / float(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient\n",
    "\n",
    "A condição de parada do nosso algoritmo é baseada no gradiente do RSS, dado pela seguinte fórmula:\n",
    "![title](img/picture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def step_gradient_vectorized(w_current,X,Y,learningRate):\n",
    "    \n",
    "    res = Y - np.dot(X, w_current) # (y−Hw)\n",
    "   \n",
    "    gradients = np.multiply(-2, np.dot(X.T, res)) # -2H.T (y−Hw)\n",
    "    \n",
    "    rate = np.multiply(learningRate, gradients)  # α * -2H.T (y−Hw)  \n",
    "    \n",
    "    new_w = w_current - rate # w(t) − α * -2H.T (y−Hw)\n",
    "    \n",
    "    return [new_w,gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    gradients = np.array([np.inf])\n",
    "    i = 0\n",
    "    while (np.linalg.norm(gradients)>=epsilon):\n",
    "        w,gradients = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        \n",
    "        #print(np.linalg.norm(gradients))\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(\"MSE in the iteration {0} is equals to {1}\".format(i,compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    print(\"It converged with {0} iterations.\".format(i))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main\n",
    "\n",
    "Como pode-se observar pela saída, o código converge com 333314 iterações quando os parâmetros são: **learning_rate = 0.00003** e **epsilon = 0.00001**. Os dois parâmetros foram sendo variados até que os coeficientes se aproximassem dos coeficientes encontrados pelo modelo criado com a biblioteca sklearn.\n",
    "\n",
    "Depois de aproximadamente 3385 ms, w0 = 1.73770337, w1 = 0.10304158, w2 = 0.04643707, w3 = 0.16409833, w4 = 0.38117884  e w5 = 0.02027826, com erro de 0.41133759. \n",
    "\n",
    "Como é possível observar, esses valores se aproximam dos valores retornados pelo modelo criado com a biblioteca sklearn, com a precisão de 4 casas decimais. Apenas o erro se aproxima com a precisão de somente 1 casa decimal.\n",
    "Relembrando, os valores do modelo do sklearn foram: w0 = 1.73771151, w1 = 0.10304143, w2 = 0.0464367, w3 = 0.16409834, w4 = 0.38117843 e w5 = 0.02027816, com erro de 0.423803240951211;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at w = [[0. 0. 0. 0. 0. 0.]], error = [[54.47995386]]\n",
      "Running...\n",
      "MSE in the iteration 0 is equals to [[15.39415211]]\n",
      "MSE in the iteration 10000 is equals to [[0.42109776]]\n",
      "MSE in the iteration 20000 is equals to [[0.41601668]]\n",
      "MSE in the iteration 30000 is equals to [[0.41358078]]\n",
      "MSE in the iteration 40000 is equals to [[0.41241299]]\n",
      "MSE in the iteration 50000 is equals to [[0.41185314]]\n",
      "MSE in the iteration 60000 is equals to [[0.41158475]]\n",
      "MSE in the iteration 70000 is equals to [[0.41145608]]\n",
      "MSE in the iteration 80000 is equals to [[0.41139439]]\n",
      "MSE in the iteration 90000 is equals to [[0.41136482]]\n",
      "MSE in the iteration 100000 is equals to [[0.41135064]]\n",
      "MSE in the iteration 110000 is equals to [[0.41134385]]\n",
      "MSE in the iteration 120000 is equals to [[0.41134059]]\n",
      "MSE in the iteration 130000 is equals to [[0.41133903]]\n",
      "MSE in the iteration 140000 is equals to [[0.41133828]]\n",
      "MSE in the iteration 150000 is equals to [[0.41133792]]\n",
      "MSE in the iteration 160000 is equals to [[0.41133775]]\n",
      "MSE in the iteration 170000 is equals to [[0.41133767]]\n",
      "MSE in the iteration 180000 is equals to [[0.41133763]]\n",
      "MSE in the iteration 190000 is equals to [[0.41133761]]\n",
      "MSE in the iteration 200000 is equals to [[0.4113376]]\n",
      "MSE in the iteration 210000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 220000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 230000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 240000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 250000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 260000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 270000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 280000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 290000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 300000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 310000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 320000 is equals to [[0.41133759]]\n",
      "MSE in the iteration 330000 is equals to [[0.41133759]]\n",
      "It converged with 333314 iterations.\n",
      "After 3385.5669498443604 ms w = [[1.73770337 0.10304158 0.04643707 0.16409833 0.38117884 0.02027826]], error = [[0.41133759]]\n"
     ]
    }
   ],
   "source": [
    "grades_incremented = np.c_[np.ones(len(grades)),grades]\n",
    "\n",
    "X = grades_incremented[:,[0,1,2,3,4,5]] # grades without cra\n",
    "Y = grades_incremented[:,6][:,np.newaxis] # cra\n",
    "init_w = np.zeros((6,1)) # vector of coefficients\n",
    "\n",
    "learning_rate = 0.00003\n",
    "epsilon = 0.00001\n",
    "\n",
    "print(\"Starting gradient descent at w = {0}, error = {1}\".format(init_w.T, compute_mse_vectorized(init_w, X,Y)))\n",
    "print(\"Running...\")\n",
    "startTime = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X,Y, learning_rate, epsilon)\n",
    "endTime = time.time()\n",
    "print(\"After {0} ms w = {1}, error = {2}\".format(str(1000*(endTime-startTime)), w.T, compute_mse_vectorized(w,X,Y)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
